{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Scene-specific_system\n",
        "\n",
        "이전에 구현 하였던 Scene-specific method 에서는 씬에따라 Detector(Head)를 유저가 선택해야한다는 불편한점을 개선하기 위하여 위의 시스템이 구현됨."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 명령어\n",
        "\n",
        "* python Cdetect.py  --weights runs/train/yolov5s_new/weights/best.pt && cd.. && cd yolov5_detect && python detect.py --weights runs/train/yolov5s_new/weights/best.pt\n",
        "\n",
        "### 또는 명령어 아이콘 클릭 \n",
        "\n",
        "click_cmd 파일 아이콘 클릭으로 실행\n",
        "\n",
        "```\n",
        "Scene-specific_system\n",
        "    --- yolov5_infer\n",
        "        --- fil1.1\n",
        "        --- fil1.2\n",
        "        --- fil1.3\n",
        "        --- click_cmd (here)\n",
        "        --- fil1.n\n",
        "        \n",
        "    --- yolov5_ detect\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "폴더는 크게 두가지로 나뉜다.\n",
        "\n",
        "1. yolov5_infer\n",
        "* yolov5_infer는 현재 들어오는 씬에서 어떠한 오브젝트 사이즈들이 분포 하는지 파악 하는 과정을 가진다.\n",
        "\n",
        "2. yolov5_detect\n",
        "* yolov5_detect는 yolov5_infer에서 파악된 과정의 결과 값을 가져와 어떠한 Detector가 현재 씬에 가장 적절한지 선택 후 추론과정을 가진다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 yolov5_infer\n",
        "\n",
        "각 오브젝트 사이즈를 지정한 Detector(head) 별 들어오는 씬의 오브젝트 전체 갯수를 Sum하고 이후 이러한 전체 갯수를 통해 각 Detector의 결과값을 가진다.\n",
        "\n",
        "File: /Scene-specific_system/yolov5_infer/Cdetect.py\n",
        "아래의 파일은 Cdetect.py의 line 52 ~ 53과 line 116 ~ 256을 나타낸다.\n",
        "\n",
        "1. line 52 ~ 53은 각 Detector의 개별 수행을 반복적으로 진행해주기 위해 checkfile이 필요하며 최종 이러한 과정일 끝난 후 결과 값을 가지기 위해 sumfile이 필요하다.\n",
        "\n",
        "\n",
        "2. line 116 ~ 256은 4번의 inference 과정을 가지게 되고 각 추론 과정 마다 위에서 정한 Detector가 순차적으로 입력된다. # Run inference  _ 1 ~ 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# line 52 ~ 53\n",
        "\n",
        "checkfile = \"C:/Users/dlwlsgh/testworks_final/Scene-specific_system/yolov5_infer/CheckFile.txt\"\n",
        "sumfile = \"C:/Users/dlwlsgh/testworks_final/Scene-specific_system/yolov5_infer/sum.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# line 116 ~ 256\n",
        "\n",
        "    # Run inference  _ 1 \n",
        "    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
        "    seen, windows, dt = 0, [], [0.0, 0.0, 0.0]\n",
        "    detection_result = 0\n",
        "    data_sum = 0\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with open(checkfile, \"w\") as file:\n",
        "            file.write(\"1\")\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize) \n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2   \n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # results of detect\n",
        "        modi = pred[0] \n",
        "        modi1 = modi.size()\n",
        "        detection_result = (modi1[0]) # 각 이미지별 디텍션 결과 갯수\n",
        "        #print(f\"1. {detection_result}\")\n",
        "        data_sum += detection_result\n",
        "\n",
        "    with open(sumfile, 'a') as file:  \n",
        "        data1 = f\"{data_sum}\\n\"              \n",
        "        file.write(str(data1))\n",
        "        data_sum = 0\n",
        "            \n",
        "    # Run inference  _ 2\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with open(checkfile, \"w\") as file:\n",
        "            file.write(\"2\")\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize) \n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2   \n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # results of detect\n",
        "        modi = pred[0] \n",
        "        modi1 = modi.size()\n",
        "        detection_result = (modi1[0]) # 각 이미지별 디텍션 결과 갯수\n",
        "        #print(f\"1. {detection_result}\")\n",
        "        data_sum += detection_result\n",
        "\n",
        "    with open(sumfile, 'a') as file:  \n",
        "        data1 = f\"{data_sum}\\n\"              \n",
        "        file.write(str(data1))\n",
        "        data_sum = 0\n",
        "    \n",
        "    # Run inference  _ 3\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with open(checkfile, \"w\") as file:\n",
        "            file.write(\"3\")\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize) \n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2   \n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # results of detect\n",
        "        modi = pred[0] \n",
        "        modi1 = modi.size()\n",
        "        detection_result = (modi1[0]) # 각 이미지별 디텍션 결과 갯수\n",
        "        #print(f\"1. {detection_result}\")\n",
        "        data_sum += detection_result\n",
        "\n",
        "    with open(sumfile, 'a') as file:  \n",
        "        data1 = f\"{data_sum}\\n\"              \n",
        "        file.write(str(data1))\n",
        "        data_sum = 0\n",
        "\n",
        "    # Run inference  _ 4\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with open(checkfile, \"w\") as file:\n",
        "            file.write(\"4\")\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize) \n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2   \n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # results of detect\n",
        "        modi = pred[0] \n",
        "        modi1 = modi.size()\n",
        "        detection_result = (modi1[0]) # 각 이미지별 디텍션 결과 갯수\n",
        "        #print(f\"1. {detection_result}\")\n",
        "        data_sum += detection_result\n",
        "\n",
        "    with open(sumfile, 'a') as file:  \n",
        "        data1 = f\"{data_sum}\\n\"              \n",
        "        file.write(str(data1))\n",
        "        data_sum = 0\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
        "                s += f'{i}: '\n",
        "            else:\n",
        "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "                   \n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(f'{txt_path}.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        #print(f\"# Add bbox to image :{c}\\n\")\n",
        "\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "                    if save_crop:\n",
        "                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                if platform.system() == 'Linux' and p not in windows:\n",
        "                    windows.append(p)\n",
        "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
        "                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "        # Print time (inference-only)\n",
        "        LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "최종 추론과정이 완료되면 File: /Scene-specific_system/yolov5_infer/sum.txt 파일에 각 Detector별 결과값이 기록된다.\n",
        "\n",
        "예시)\n",
        "1. 명령어\n",
        "* python Cdetect.py  --weights runs/train/yolov5s_new/weights/best.pt\n",
        "\n",
        "2. txt 결과 값\n",
        "* 2623\n",
        "* 2806\n",
        "* 2916\n",
        "* 2942"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 yolov5_detect\n",
        "\n",
        "yolov5_infer에서 추론된 txt 결과 값을 통해 현재 씬에서 무엇이 가장 적절한지 판단함.\n",
        "이러한 과정을 수행하기 위해 위의 txt 결과값을 퍼센트 단위로 변경함.\n",
        "\n",
        "File: /Scene-specific_system/yolov5_detect/yolo.py\n",
        "아래의 파일은 Cdetect.py의 line 43 ~ 51을 나타낸다.\n",
        "\n",
        "- txt 파일을 yolo에서 불러온 이후 percent 단위로 변경.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: /Scene-specific_system/yolov5_detect/yolo.py (line 43 ~ 51)\n",
        "\n",
        "inferPath = \"C:/Users/dlwlsgh/testworks_final/Scene-specific_system/yolov5_infer/sum.txt\"\n",
        "\n",
        "with open(inferPath, \"r\") as file:\n",
        "    strings = file.readlines()\n",
        "    temp = list(map(int, strings))\n",
        "    \n",
        "\n",
        "# temp [0], [1], [2] ,[3] = head1, head2, head3, head4(all)\n",
        "h1, h2, h3, h4 = percentCompare(temp[0], temp[1], temp[2], temp[3])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "File: /Scene-specific_system/yolov5_detect/utils/function.py\n",
        "아래의 파일은function.py의 line 54 ~ 59를 나타낸다.\n",
        "\n",
        "이후 txt 파일의 결과 값을 4개의 헤드를 전부 사용하는 Detector를 기준으로 퍼센트 변환과정을 거친다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: /Scene-specific_system/yolov5_detect/utils/function.py (line 54 ~ 59)\n",
        "\n",
        "def percentCompare(h1, h2, h3, h4):\n",
        "     h1 = round(h1 / h4 * 100)\n",
        "     h2 = round(h2 / h4 * 100)\n",
        "     h3 = round(h3 / h4 * 100)\n",
        "     h4 = round(h4 / h4 * 100)\n",
        "     return h1, h2, h3, h4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이후 결과값은 서로 비교하는 과정을 가진다. \n",
        "전체 헤드를 사용하는 Detector를 기준으로 5% 이내일 경우 또는 3% 등 아래부터 순차적으로 선택된다.\n",
        "\n",
        "File: /Scene-specific_system/yolov5_detect/yolo.py\n",
        "아래의 파일은 Cdetect.py의 line 53 ~ 76을 나타낸다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hSum = [h1, h2, h3, h4]\n",
        "\n",
        "print(hSum)\n",
        "\n",
        "nano_ = h4 - h1\n",
        "small_ = h4 - h2\n",
        "medium_ = h4 - h3\n",
        "compa_medium = h3 - h2\n",
        "\n",
        "if nano_ <= 3: \n",
        "    start, end, head = 22, 30, 1\n",
        "    print(\"Nano\")\n",
        "elif small_ <= 3:\n",
        "    start, end, head = 25, 30, 2\n",
        "    print(\"Small\")\n",
        "elif compa_medium <= 1:\n",
        "    start, end, head = 25, 30, 2\n",
        "    print(\"Small\")     \n",
        "elif medium_ <= 5:\n",
        "    start, end, head = 28, 30, 3 \n",
        "    print(\"Medium\")     \n",
        "else: \n",
        "    start, end, head = 0, 0, 1\n",
        "    print(\"Large\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "## 2. 시작전 확인 사항\n",
        "\n",
        "경로의 시작점은 항상 \"Scene-specific_system\\yolov5_infer\"의 경로에서 전체 명령어가 수행되어야 함\n",
        "- 전체 명령어 예제\n",
        "- - testworks_final\\Scene-specific_system\\yolov5_infer> python Cdetect.py  --weights runs/train/yolov5s_new/weights/best.pt && cd.. && cd yolov5_detect && python detect.py --weights runs/train/yolov5s_new/weights/best.pt\n",
        "\n",
        "수행이 정상적으로 진행되어 있지 않은 경우 yolov5_infer폴더 내 sum.txt 내에서 모든 값을 remove 해줘야 함. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 클릭 커맨드\n",
        "\n",
        "click_cmd 파일 아이콘 클릭으로 실행\n",
        "\n",
        "```\n",
        "Scene-specific_system\n",
        "    --- yolov5_infer\n",
        "        --- click_cmd\n",
        "\n",
        "\n",
        "    --- yolov5_ detect\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "290d2e246ddb8b1fcf65ef11241e81796f33989a55bf90b94a7afd332cb4d840"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20c89dc0d82a4bdf8756bf5e34152292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801e720897804703b4d32f99f84cc3b8",
            "placeholder": "​",
            "style": "IPY_MODEL_c9fb2e268cc94d508d909b3b72ac9df3",
            "value": "100%"
          }
        },
        "572de771c7b34c1481def33bd5ed690d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20c89dc0d82a4bdf8756bf5e34152292",
              "IPY_MODEL_61026f684725441db2a640e531807675",
              "IPY_MODEL_8d2e16d90e13449598d7b3fac75f78a3"
            ],
            "layout": "IPY_MODEL_a09d90f1bd374ece9a29bc6cfe07c072"
          }
        },
        "61026f684725441db2a640e531807675": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfbc16e88df24fae93e8c80538e78273",
            "max": 818322941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9ffa50bddb7455ca4d67ec220c4a10c",
            "value": 818322941
          }
        },
        "78e5b8dba72942bfacfee54ceec53784": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "801e720897804703b4d32f99f84cc3b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8be83ee30f804775aa55aeb021bf515b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2e16d90e13449598d7b3fac75f78a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be83ee30f804775aa55aeb021bf515b",
            "placeholder": "​",
            "style": "IPY_MODEL_78e5b8dba72942bfacfee54ceec53784",
            "value": " 780M/780M [01:28&lt;00:00, 9.08MB/s]"
          }
        },
        "a09d90f1bd374ece9a29bc6cfe07c072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfbc16e88df24fae93e8c80538e78273": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fb2e268cc94d508d909b3b72ac9df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9ffa50bddb7455ca4d67ec220c4a10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
